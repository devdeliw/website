[{"id":0,"href":"/resume/","title":"Resume","section":"my undergrad work","content":"test\n"},{"id":1,"href":"/posts/coral_intro/","title":"What is CORAL?","section":"Writeups","content":" In the past few months I\u0026rsquo;ve become immensely interested in scientific computing and writing fast code. I started CORAL as a project to learn both at the same time. And learn Rust.\nCORAL stands for COre Rust Architecture for Linear algebra. It is an implementation of the Basic Linear Algebra Subprograms, or BLAS, in pure Rust. It is written for AArch64 architectures only.\nBLAS is the set of the most common low-level operations, \u0026ldquo;kernels\u0026rdquo;, for linear algebra. Most numerical routines involve linear algebra; it is clear that a useful BLAS must be as fast as possible. These kernels naturally separate into three levels, each monumentally more difficult than the last.\nLevel 1# Vector-Vector Operations Think of things like calculating the dot product, $\\vec{x} \\cdot \\vec{y}$, or multiplying by a scalar, $\\alpha \\vec{x}$. These operations are memory bound; the bottleneck is how fast memory is moved around, not how fast the CPU is. Good performance can be achieved if code is written intelligently.\nLevel 2# Matrix-Vector Operations Think of things like calculating $A\\vec{x}$, or solving a system of equations $A\\vec{x} = \\vec{b}$ given a triangular matrix $A$ and $\\vec{b}$. These operations are also memory bound. It is here though, that clever tricks leveraging cache to maximize performance begin. Good performance can still be achieved with smart code.\nLevel 3# Matrix-Matrix Operations Think of things like calculating $AB$. It\u0026rsquo;s fair to say $AB$ is the most executed mathematical operation on the planet. It is also compute bound, which means reaching peak performance is still an active area of research.\nA BLAS\u0026rsquo;s performance is almost entirely dependent on how fast it can calculate $AB$. Consequently, solving many $AB$s is how supercomputers are benchmarked today. AI only exists today because matrix multiplication became fast enough.\nOne of BLAS\u0026rsquo;s pioneers is Kazushige Goto, who hand optimized assembly routines for his GotoBLAS. This implementation outperformed many BLAS used at the time and became the backbone for the current industry standard OpenBLAS. If you use Python and NumPy for vector calculations, OpenBLAS is why it\u0026rsquo;s so fast.\nCORAL isnâ€™t built to compete with industry BLAS, but to reach $\\simeq$ 80 % of their performance on AArch64.$^\\dagger$ This is to educate myself and others on how these fast low-level algorithms work. The purpose of this \u0026ldquo;blog\u0026rdquo; is to walk through how to intelligently write code to make a fast BLAS. Just not one that\u0026rsquo;s used by supercomputers.\n$^\\dagger$ Turns out, on AArch64, CORAL is actually comparable to OpenBLAS. CORAL outperforms for DGEMM (calculating $C \\leftarrow \\alpha AB + \\beta C$ in double precision). You can see the benchmark here. However, on Apple Silicon, Apple Accelerate, another BLAS implementation, absolutely wrecks both CORAL and OpenBLAS.\n"}]