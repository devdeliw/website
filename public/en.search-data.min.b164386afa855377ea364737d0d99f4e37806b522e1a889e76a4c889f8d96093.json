[{"id":0,"href":"/optimizing/aoc/","title":"Advent of Code","section":"optimizing","content":"2025# Day 02\n"},{"id":1,"href":"/posts/benchmarks/","title":"coral benchmarks","section":"posts","content":" Apple M4 (6P + 4E), 16GB unified memory. All benchmarks are single-precision and single-threaded.\nEach plot shows:\ncoral-safe (portable-simd, safe Rust) coral-neon (AArch64 / NEON) a reference implementation: OpenBLAS armv8, or Apple Accelerate, or BLIS for sgemm faer for sgemm/matmul The OpenBLAS backend used is optimized for Level2-3. For some Level1 routines like SNRM2, I believe this backend just uses the reference, netlib algorithm.\nTable of Contents# OpenBLAS Level 1 Level 2 Level 3 With Apple Accelerate Level 1 Level 2 Level 3 OpenBLAS# Level 1# ISAMAX — index of max absolute value# SASUM — sum of absolute values# SAXPY — scalar vector addition# SCOPY — copy vector into another# SDOT — dot product# SNRM2 — Euclidean norm# SROT — Givens rotation# SROTM — modified Givens rotation# SSCAL — scale vector# SSWAP — swap two vectors# Level 2# SGEMV — matrix–vector multiply# \\[ y \\leftarrow \\alpha \\operatorname{op}(A)x + \\beta y \\]\nSGER — rank-1 update# \\[ A \\leftarrow \\alpha x y^T + A \\]\nSSYMV — symmetric matrix–vector multiply# \\[ y \\leftarrow \\alpha A x + \\beta y, \\quad A = A^T \\]\nstored lower triangle: stored upper triangle: SSYR — symmetric rank-1 update# \\[ A \\leftarrow \\alpha x x^T + A \\]\nlower triangle stored: upper triangle stored: SSYR2 — symmetric rank-2 update# \\[ A \\leftarrow \\alpha (x y^T + y x^T) + A \\]\nlower triangle stored: upper triangle stored: STRMV — triangular matrix–vector multiply# \\[ x \\leftarrow \\operatorname{op}(A) x \\]\nUpper triangular (STRMV):\nLower triangular (STRMV):\nSTRSV — triangular solve# \\[ x \\leftarrow A^{-1} b \\]\nUpper triangular (STRSV):\nLower triangular (STRSV):\nLevel 3# SGEMM — matrix–matrix multiply# \\[ C \\leftarrow \\alpha \\operatorname{op}(A)\\operatorname{op}(B) + \\beta C \\]\nApple Accelerate# The following benchmarks are the same as above, but with Apple Accelerate. For critical routines like sgemv and sgemm, Apple uses AMX to be much faster. Consequently it masks any comparison between my coral implementations and other BLAS.\nLevel 1 (Accelerate)# ISAMAX — index of max absolute value# SASUM — sum of absolute values# SAXPY — scaled vector addition# SCOPY — copy a vector into another# SDOT — dot product# SNRM2 — Euclidean norm# SROT — Givens rotation# SROTM — modified Givens rotation# SSCAL — scale a vector# SSWAP — swap two vectors# Level 2 (Accelerate)# SGEMV — matrix–vector multiply# SGER — rank-1 update# SSYMV — symmetric matrix–vector multiply# Lower triangle stored:\nUpper triangle stored:\nSSYR — symmetric rank-1 update# Lower triangle stored:\nUpper triangle stored:\nSSYR2 — symmetric rank-2 update# Lower triangle stored:\nUpper triangle stored:\nSTRMV — triangular matrix–vector multiply# Upper triangular (STRMV):\nLower triangular (STRMV):\nSTRSV — triangular solve# Upper triangular (STRSV):\nLower triangular (STRSV):\nLevel 3 (Accelerate)# SGEMM — matrix–matrix multiply# "},{"id":2,"href":"/posts/benchmarks-aarch64/","title":"coral-aarch64 benchmarks","section":"posts","content":" Apple M4 (6P + 4E), 16GB unified memory. All benchmarks are single-threaded.\nIn all plots, CORAL is benchmarked against OpenBLAS. Some routines also include Apple Accelerate. When Accelerate is omitted, it\u0026rsquo;s because its AMX-backed kernels on this M4 MacBook Pro are much faster and mask any comparison with OpenBLAS.\nFor sgemm, faer is included, also single-threaded.\nTable of Contents# Level 1 AXPY SCAL DOT Level 2 GEMV TRSV TRMV Level 3 GEMM Level 1# AXPY# AXPY performs a scaled vector addition: \\[ y \\leftarrow \\alpha x + y \\]\nf32# f64# c32# c64# SCAL# SCAL scales a vector by a scalar: \\[ x \\leftarrow \\alpha x \\]\nf32# f64# c32# c64# DOT# Real dot product: \\[ \\operatorname{dot}(x, y) = \\sum_i x_i y_i \\]\nComplex variants:\nconjugated: \\(\\operatorname{dotc}(x, y) = \\sum_i \\overline{x_i} y_i\\) unconjugated: \\(\\operatorname{dotu}(x, y) = \\sum_i x_i y_i\\) f32# f64# c32# conj (cdotc)# unconj (cdotu)# c64# conj (zdotc)# unconj (zdotu)# Level 2# GEMV# Matrix–vector multiply: \\[ y \\leftarrow \\alpha \\operatorname{op}(A) x + \\beta y, \\quad \\operatorname{op}(A) \\in {A, A^T, A^H} \\]\nf32# f64# c32# c64# TRSV# Triangular solve: \\[ x \\leftarrow A^{-1} b, \\quad A \\text{ triangular} \\]\nf32# LOWER TRIANGULAR# UPPER TRIANGULAR# f64# LOWER TRIANGULAR# UPPER TRIANGULAR# c32# LOWER TRIANGULAR# UPPER TRIANGULAR# c64# LOWER TRIANGULAR# UPPER TRIANGULAR# TRMV# Triangular matrix–vector multiply: \\[ x \\leftarrow \\operatorname{op}(A) x, \\quad A \\text{ triangular} \\]\nf32# LOWER TRIANGULAR# UPPER TRIANGULAR# f64# LOWER TRIANGULAR# UPPER TRIANGULAR# c32# LOWER TRIANGULAR# UPPER TRIANGULAR# c64# LOWER TRIANGULAR# UPPER TRIANGULAR# Level 3# GEMM# Matrix–matrix multiply: \\[ C \\leftarrow \\alpha \\operatorname{op}(A)\\operatorname{op}(B) + \\beta C, \\quad \\operatorname{op}(A), \\operatorname{op}(B) \\in {,\\cdot, {}^T, {}^H} \\]\nf32# f64# c32# "},{"id":3,"href":"/posts/memory/","title":"Understanding Memory","section":"posts","content":" Before diving into how BLAS is written to be fast, it\u0026rsquo;s essential to understand memory. Specifically, how data is stored in memory and how it is fed to the processor, which performs the computations. All BLAS does is optimize these two operations for a specific computer architecture. CORAL, for instance, targets AArch64 architectures.\nFuture posts on BLAS will refer back to concepts explained here.\nThe content in this post is heavily taken from What Every Programmer Should Know About Memory by Ulrich Drepper. It\u0026rsquo;s phenomenal. Unless otherwise linked, all numerical values come from this paper.\nHeuristics Ahead Cycle counts, sizes, and policies shown here are approximate heuristics.\nThey vary by CPU architecture and memory configuration.\nMemory# Memory is a physical structure that stores digital information as electrical states. A single bit of memory represents either a logical 0 or 1 by maintaining distinct, measurable voltages inside microscopic circuits. These circuits exist in layers of memory hierarchy, a set of memory storage levels that trade off speed, capacity, and energy.\nA fast BLAS aims to keep its working data in the fastest available level of the hierarchy.\nDefinition A clock cycle is one complete oscillation of a processor\u0026rsquo;s clock signal. It\u0026rsquo;s the periodic rise and fall of voltage that synchronizes every operation in the CPU \u0026ndash; the smallest unit of computational time.\nAt the lowest level are registers inside the CPU itself. Accessing data from registers takes $\\leq 1$ cycle. Above registers are caches, which exist in three sub-levels. Accessing cached data takes between $\\sim$3-60 cycles depending on which sub-level is accessed. Beyond caches lies the main memory. Accessing data from main memory takes $\\sim$200-400 cycles. Together these levels form the hierarchy that moves data from storage to the processor.\nNaturally, the lower level storages are faster to read and write data to. However, they are much smaller in size, trading capacity for speed. This will be critical when discussing packing strategies for BLAS.\nStatic and Dynamic RAM# All working memory in a computer is built on one of two forms of Random Access Memory (RAM): Static RAM (SRAM) and Dynamic RAM (DRAM). Let\u0026rsquo;s discuss why there are two forms at all, understand how they work, and their read/write speeds.\nSRAM# This is one SRAM cell that can store a 0 or 1. The core of the cell is formed by four transistors $M_1$ to $M_4$ that form two cross-coupled inverters (NOT gates). This means each inverter output feeds the input of the other, shown below.\nThis loop, where the output of one inverter drives the input of the other, creates bistability. The mutual inversion forms a feedback loop that can hold one of two stable states, in the form of low or high voltages:\n$$ \\begin{align*} Q = 1,\\quad \u0026amp;\\bar{Q} = 0 \\\\ Q = 0,\\quad \u0026amp;\\bar{Q} = 1 \\end{align*} $$\nThis is the foundation of SRAM. These states can be held indefinitely as long as power $V_{dd}$ is available to the cell \u0026ndash; SRAM is static.\nDRAM# DRAM, in its structure, is much simpler than SRAM. It consists of a single transistor $M$ and capacitor $C$. A DRAM cell keeps its state in $C$. $M$ is used to guard access to the state.\nA stored 1 means the capacitor is charged; a 0 means discharged. However, a modern DRAM capacitor is tiny \u0026ndash; on the order of 20-40 femtofarads. To store a 1, a charge of $\\sim 36 \\times 10^{-15} C$ is placed on the capacitor. That\u0026rsquo;s 36 femtocoulombs, or about 225,000 electrons.\nCapacitors discharge or \u0026ldquo;leak\u0026rdquo;. Every $\\sim 64$ms they have to be refreshed/recharged. This doesn\u0026rsquo;t stall the whole memory, but it does take up some cycles. To read data on the capacitor also requires discharging it, which takes more cycles. But now the charge on the capacitor is depleted, so every read also is followed by an operation to recharge the capacitor, which takes even more cycles.\nDRAM cells, however, are much smaller than SRAM cells. Packing many DRAM cells close together is much simpler, and less expensive. For this reason, caches use SRAM cells, where speed is critical. Main memory uses DRAM cells, where capacity is critical. A typical L1 cache may contain hundreds of thousands of SRAM cells, whereas main memory contains trillions of DRAM cells.\nAccessing Memory# Both SRAM and DRAM cells are arranged in a 2D array. To access a row, the wordline $WL$ is raised. It\u0026rsquo;s a conductive path shared by all cells in a row.\nIn SRAM, this opens two access transistors \u0026ndash; $M_5$ and $M_6$ that allow the SRAM states to be read or written to via the bitline $BL$.\nIn DRAM, the raised wordline instead activates the single access transistor $M$ that connects the capacitor $C$ to its bitline.\nReading or writing a bit in either SRAM or DRAM roughly follows these steps:\nSelect a Row (WL) The cache controller raises a wordline. In SRAM this opens two access transistors. In DRAM this opens a single access transistor.\nConnect to the Bitlines (BL) Each column of cells shares a bitline. When connected, the cell either slightly raises or slightly lowers the voltage on that bitline depending on whether it stores 0 or 1.\nIn SRAM, this is fast. In DRAM, this takes more time (discharging and charging capacitors).\nSense Voltage Differences The voltage change is incredibly small, only a few millivolts. Nearby sense amplifiers detect which direction (increase or decrease) the voltage change occurred. Then they amplify the difference into a full digital 0V or 1V signal.\nDeliver to the CPU The sensed bits are combined into a word (for example, 64 bits), and transferred along metal interconnects towards the CPU\u0026rsquo;s registers. Inside the processor, these signals are now conventional digital values, ready for arithmetic.\nThe Cache# The gap between CPU speed and DRAM speed is vast. Modern processors can perform hundreds of arithmetic operations in the time it takes to fetch one value from main memory. To close this gap, CPUs use multiple levels of cache memory built from SRAM.\nCaches hold recently used data so that repeated accesses are fast.\n$$ \\begin{align*} \u0026amp;\\text{Level} \u0026amp;\u0026amp;\\text{Composition} \u0026amp;\u0026amp;\u0026amp;\\text{Access Time} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{Typical Size} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{Scope} \\\\ \u0026amp;\\text{L1 Cache} \u0026amp;\u0026amp;\\text{SRAM} \u0026amp;\u0026amp;\u0026amp;\\sim\\text{3 cycles} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{32-64KB} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{Per core} \\\\ \u0026amp;\\text{L2 Cache} \u0026amp;\u0026amp;\\text{SRAM} \u0026amp;\u0026amp;\u0026amp;\\sim\\text{10 cycles} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{256KB-1MB} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{Per core or cluster} \\\\ \u0026amp;\\text{L3 Cache} \u0026amp;\u0026amp;\\text{SRAM} \u0026amp;\u0026amp;\u0026amp;\\sim\\text{30-60 cycles} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{4-64MB} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{Shared among cores} \\\\ \u0026amp;\\text{Main Memory} \u0026amp;\u0026amp;\\text{DRAM} \u0026amp;\u0026amp;\u0026amp;\\sim\\text{200-400 cycles} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{GBs} \u0026amp;\u0026amp;\u0026amp;\u0026amp;\u0026amp;\\text{System-wide} \\end{align*} $$\nEach level acts as a local buffer for the one beneath it, ensuring that frequently used data remains as close to the CPU as possible.\nWhen the CPU requests data, it first searches through the caches. If it\u0026rsquo;s found in L1, that\u0026rsquo;s an L1 cache hit, and optimal. If it\u0026rsquo;s not found in L1, it searches through L2, then L3. If it\u0026rsquo;s not found in L1-L3, it\u0026rsquo;s a cache miss, and the data must be fetched from main-memory; much slower.\nWriting to cache often requires making room. When space is needed, modified lines are written back to L2 on line eviction, which may in turn write to L3, and so on. Each write-back takes clock cycles.\nThe difficulty in writing a fast BLAS is fitting as much as possible in L1-L2 and minimizing the number of cycles used to move memory around. This is discussed below.\nImplementation# Each cache level (L1, L2, L3) is implemented as a 2D array of cache lines, just like how SRAM and DRAM are organized into rows and columns. However, unlike with DRAM, cache must be able to quickly decide whether a given memory address (that points to requested physical memory) is already stored inside it, and if so, where. This lookup must happen in as few cycles as possible.\nDesigns# When the CPU requests a memory address, the cache must check if that address exists in its storage. The address bits of any memory request are divided into three parts that tell the cache exactly where to look and what to verify.\n$$ \\begin{align*} \\text{Address Bits} = [\\text{ Tag } | \\text{ Set Index } | \\text{ Block Offset }] \\end{align*} $$\nDefinition The Block Offset bits specify the location of the exact byte within a single cache line. Once the correct line is found, these bits identify the requested byte in the line.\nDefinition The Set Index bits choose which \u0026ldquo;set\u0026rdquo;, or horizontal row of cache lines to check. Each set contains several \u0026ldquo;ways\u0026rdquo; (typically 4-16 lines) that can all hold different blocks of memory.\nOnly the lines in this one set are searched; the rest of the cache is ignored.\nDefinition The tag bits identify which block of main memory the requested cache line is from. When the CPU looks up an address, the tag from that address is compared to the stored tags of all ways in the chosen set.\nIf a tag matches, it\u0026rsquo;s a cache hit. If no tag matches, it\u0026rsquo;s a cache miss.\nHits and Misses# If a tag matches in L1, then L1 already holds the 64-byte block containing the requested address. The cache uses the Block Offset bits to select the exact bytes needed, and the data is returned to the CPU within $\\sim$3 cycles.\nIf no tag matches, it’s a cache miss. The cache must fetch the entire 64-byte block from the next level of the hierarchy (L2, L3, or DRAM). This process unfolds roughly as follows:\nMiss detected (1-2 cycles) The cache controller recognizes the tag is not present and signals a miss\nNext-level lookup ($\\sim$10 cycles for L2) The controller forwards the request to the next cache level. If L2 holds the block, it\u0026rsquo;s returned and placed in L1.\nPropagated miss ($\\sim$30-60 cycles for L3) If L2 also misses, the request cascades to L3, losing more cycles.\nMain memory access ($\\sim$200-400 cycles) If all cache levels miss, the controller fetches the block from DRAM. This is hundreds of cycles slower than an L1 hit.\nLine replacement ($\\sim$1-2 cycles) Once the data arrives, the cache writes it to an appropriate set. If the set is full, one line is evicted.\nRetry (1 cycle) The CPU re-attempts the load, which now hits in L1.\nThe key takeaway is the huge latency difference between L1/L2 hits and misses. It can stall the processor for tens to hundreds of cycles, depending on how deep the request travels down the hierarchy. Mastering this gap \u0026ndash; between a few cycles for a hit and hundreds for a miss \u0026ndash; is the essence of high-performance BLAS. It\u0026rsquo;s what makes cache efficiency so critical for CPU performance and writing fast code.\nLocality and Contiguity# I hope I\u0026rsquo;ve made it very clear that performance depends on how data is arranged in memory. If values that are used together are also stored together, the CPU can reuse cache lines efficiently, producing many hits per fetch.\nTwo principles govern this:\nSpatial Locality: data near recently used data is likely to be used soon. Temporal Locality: data recently used will likely be used again. For example, consider a for loop:\nlet x: Vec\u0026lt;f32\u0026gt; = vec![1.0, 2.0, 3.0, 4.0]; for \u0026amp;val in \u0026amp;x { func(\u0026amp;val) // a function call } The vector x is a contiguous heap array of 4 f32s, 16 bytes total. Since each cache line is 64 bytes, all four elements of x reside inside the same line. Then, when the loop first loads x[0], the entire 64-byte line is fetched into L1. Then, accesses to x[1], x[2], x[3] will hit in L1 because they\u0026rsquo;re in that same cache line \u0026ndash; spatial locality.\nfunc() is a function that may be distant from x in memory. But because calls to func() are close in time, its instructions are loaded in L1 as well (assuming they fit)\u0026ndash; temporal locality.\nPause Here The above two paragraphs are the most important to understand in this post. Spatial and temporal locality are the twin principles every high-performance program is built around.\nThere are two types of L1 caches per CPU core:\nL1d \u0026ndash;data cache (for loads/stores like the vector x) L1i \u0026ndash;instruction cache (for fetched and decoded instructions like func) Summary# Memory stores data as stable voltage states in physical circuits. SRAM retains its state through feedback; DRAM requires refresh. Caches made of SRAM bridge the speed gap between DRAM and the CPU. Cache lines exist to exploit spatial and temporal locality. BLAS leverages these properties by reorganizing data to maximize reuse. Fast code is not just fast arithmetic. It is the precise choreography of data as it moves from memory to computation and back.\nFor a deeper understanding, I highly recommend reading through What Every Programmer Should Know About Memory by Ulrich Drepper. This post was abstracted and focused on cache mechanics. Drepper\u0026rsquo;s paper offers a more in-depth, elegant overview of how memory systems operate.\n← Previous "},{"id":4,"href":"/posts/coral_intro/","title":"What is CORAL?","section":"posts","content":" In the past few months I\u0026rsquo;ve become immensely interested in scientific computing and writing fast code. I started CORAL as a project to learn both at the same time. And learn Rust.\nCORAL stands for COre Rust Architecture for Linear algebra. It is an implementation of the Basic Linear Algebra Subprograms, or BLAS, in pure Rust. It is written for AArch64 architectures only.\nBLAS is the set of the most common low-level operations, \u0026ldquo;kernels\u0026rdquo;, for linear algebra. Most numerical routines involve linear algebra; it is clear that a useful BLAS must be as fast as possible. These kernels naturally separate into three levels, each monumentally more difficult than the last.\nLevel 1# Vector-Vector Operations Think of things like calculating the dot product, $\\vec{x} \\cdot \\vec{y}$, or multiplying by a scalar, $\\alpha \\vec{x}$. These operations are memory bound; the bottleneck is how fast memory is moved around, not how fast the CPU is. Good performance can be achieved if code is written intelligently.\nLevel 2# Matrix-Vector Operations Think of things like calculating $A\\vec{x}$, or solving a system of equations $A\\vec{x} = \\vec{b}$ given a triangular matrix $A$ and $\\vec{b}$. These operations are also memory bound. It is here though, that clever tricks leveraging cache to maximize performance begin. Good performance can still be achieved with smart code.\nLevel 3# Matrix-Matrix Operations Think of things like calculating $AB$. It\u0026rsquo;s fair to say $AB$ is the most executed mathematical operation on the planet. It is also compute bound, which means reaching peak performance is still an active area of research.\nA BLAS\u0026rsquo;s performance is almost entirely dependent on how fast it can calculate $AB$. Consequently, solving many $AB$s is how supercomputers are benchmarked today. AI only exists today because matrix multiplication became fast enough.\nOne of BLAS\u0026rsquo;s pioneers is Kazushige Goto, who hand optimized assembly routines for his GotoBLAS. This implementation outperformed many BLAS used at the time and became the backbone for the current industry standard OpenBLAS. If you use Python and NumPy for vector calculations, OpenBLAS is why it\u0026rsquo;s so fast.\nCORAL isn’t built to compete with industry BLAS, but to reach $\\simeq$ 80 % of their performance on AArch64.$^\\dagger$ This is to educate myself and others on how these fast low-level algorithms work. The purpose of this \u0026ldquo;blog\u0026rdquo; is to walk through how to intelligently write code to make a fast BLAS. Just not one that\u0026rsquo;s used by supercomputers.\n$^\\dagger$ Turns out, on AArch64, CORAL is actually comparable to OpenBLAS when both are single-threaded. CORAL outperforms for D/C/ZGEMM, and is $\\sim$comparable for SGEMM (single precision general matrix multiplication). This makes sense, since SGEMM is the most used. You can see the benchmark(s) here. However, optimized for Apple Silicon, Apple Accelerate, another BLAS implementation, absolutely wrecks both CORAL and OpenBLAS.\nNext → "},{"id":5,"href":"/posts/nasa_programming/","title":"Hamiltonian Programming","section":"posts","content":" This is a long post. On mobile some equations may go overfull.\nThe goal of this post is to elegantly program the Spin Hamiltonian governing recombination in Silicon Carbide. Our system involves two electrons and two nuclei.\nWe define our orthonormal basis as follows:\nThe ${\\uparrow, \\downarrow}$ spin basis is called the Zeeman basis. I define the basis with the two electrons coupled and the nuclei in the Zeeman Basis as the coupled basis. Every state in our two-electron + two-nuclei system is given by\n$$ \\underbrace{|s, m\\rangle}_{\\text{electrons}} \\underbrace{|m_{I_1}\\rangle |m_{I_2}\\rangle}_{\\text{nuclei}}. $$\nWe will evaluate $\\hat{H}_Z, \\hat{H}_{HF}, \\hat{H}_{ZFS}$, and $\\hat{H}_{EX}$ on the above state(s).\nZeeman# The Zeeman Hamiltonian, from Part 1, is defined in the coupled basis via\n$$ \\begin{align} \u0026amp;\\hat{H}_Z|s, m\\rangle |m_{I_1}\\rangle |m_{I_2}\\rangle = \\\\ \u0026amp;\\underbrace{\\left( mg_e \\mu_B B_0 + m_{I_1} g_{n_1} \\mu_N B_0 + m_{I_2} g_{n_2} \\mu_N B_0\\right)}_{\\text{energies}}|s, m\\rangle |m_{I_1}\\rangle |m_{I_2}\\rangle. \\end{align} $$\nHere, $g_e$ is $\\simeq g$-factor for the electron, $g_{n_1}$ is the $g$-factor for the first nucleus (silicon), and $g_{n_2}$ is the $g$-factor for the second nucleus (carbon). $\\mu_B$ and $\\mu_N$ are the Bohr Magneton and Nuclear Magneton respectively.\nImplementation# The coupled-basis is itself the eigenbasis. To build the Zeeman Hamiltonian, we first define the coupled basis:\nimport sympy as sp // electron |s, m\u0026gt; pairs electron_pairs = [ (1, +1), (1, 0), (0, 0), (1, -1), ] // nuclear |mI1\u0026gt;|mI2\u0026gt; pairs nuclear_pairs = [ ( sp.Rational(+1,2), sp.Rational(+1,2) ), ( sp.Rational(+1,2), sp.Rational(-1,2) ), ( sp.Rational(-1,2), sp.Rational(+1,2) ), ( sp.Rational(-1,2), sp.Rational(-1,2) ), ] Now we apply Equation (1-2) to every basis state; every combination of the above electron + nuclei pairs. This will generate our 16 $\\times$ 16 Zeeman Hamiltonian. It will be a diagonal matrix with entries because the coupled-basis is already the eigenbasis.\n// zeeman constants g_e = sp.symbol(\u0026quot;g_e\u0026quot;) g_n1 = sp.symbol(\u0026quot;g_n1\u0026quot;) // ... // zeeman frequencies omega_e = g_e * mu_B * B0 omega_n1 = g_n1 * mu_N * B0 omega_n2 = g_n2 * mu_N * B0 energies = [] for s, m in electron_pairs: // electrons run slow for mI1, mI2 in nuclear_pairs: // nuclei run fast energies.append( m*omega_e + // electron mI1*omega_n1 + // nucleus 1 mI2*omega_n2 // nucleus 2 ) // zeeman hamiltonian H_Z = sp.diag(*energies) However, we have to be careful. We rendered the Zeeman Hamiltonian in a nested for-loop over the coupled-basis states. This ordering needs to be immutable for calculating all Hamiltonians.\nHyperfine# The Hyperfine Hamiltonian is simplest when all electrons and nuclei are in the Zeeman $\\{ \\uparrow, \\downarrow \\}$ basis. We\u0026rsquo;ll first attack solving a two-electron ($a, b$) + one-nucleus ($I$) system in Zeeman basis. Then we\u0026rsquo;ll expand to the two-electron + two-nuclei system and ultimately convert to the coupled-basis at the end via a Clebsch-Gordan transformation.\nTwo electrons and one nucleus# The Hyperfine Hamiltonian, from Part 1, is defined as\n$$ \\begin{align} \\hat{H}_{HF} = \\hat{S}_a \\cdot A_a \\cdot \\hat{I} + \\hat{S}_b \\cdot A_b \\cdot \\hat{I}, \\end{align} $$\nfor a two-electron ($a, b$) and one-nuclei ($I$) system. This is equivalent to\n$$ \\begin{align} \\hat{H}_{HF} = \\sum_{k \\in \\{a, b\\}} A_{kx} S_{kx} I_x + A_{ky}S_{ky}I_y + A_{kz}S_{kz}I_z. \\end{align} $$\nWe use ladder operators $\\hat{S}_+ $ and $\\hat{S}_- $ that give\n$$ \\hat{S}_x = \\frac{1}{2}\\left(\\hat{S}_+ + \\hat{S}_- \\right) \\quad \\hat{S}_y = \\frac{1}{2i} \\left(\\hat{S}_+ - \\hat{S}_-\\right), $$\nand likewise for $\\hat{I}_x$ and $\\hat{I}_y$. Plugging this into Equation (4) yields\n$$ \\begin{align*} \\hat{H}_{HF} \u0026amp;= \\sum_k A_{kx} \\left(\\frac{1}{2}(\\hat{S}_{k+} + \\hat{S}_{k-}) \\right)\\left( \\frac{1}{2} ( \\hat{I}_+ + \\hat{I}_- ) \\right) \\\\ \u0026amp;\\qquad +A_{ky} \\left( \\frac{1}{2i} (\\hat{S}_{k+} - \\hat{S}_{k-} ) \\right) \\left( \\frac{1}{2i} (\\hat{I}_+ - \\hat{I}_-) \\right) + A_{kz} S_{kz} I_z \\\\ \u0026amp;= \\sum_k \\frac{A_{kx}}{4} ( \\hat{S}_{k+} \\hat{I}_+ + \\hat{S}_{k+} \\hat{I}_- + \\hat{S}_{k-} \\hat{I}_+ + \\hat{S}_{k-} \\hat{I}_-) \\\\ \u0026amp;\\qquad - \\frac{A_{ky}}{4} (\\hat{S}_{k+}\\hat{I}_+ - \\hat{S}_{k+}\\hat{I}_- - \\hat{S}_{k-}\\hat{I}_+ + \\hat{S}_{k-}\\hat{I}_- ) + A_{kz} S_{kz} I_z. \\end{align*} $$\nWe are almost done. From ladder operators, we know\n$$ \\begin{align} \\hat{S}_{\\pm} |s, m\\rangle = \\hbar \\sqrt{s (s+1) - m(m\\pm 1)} |s, m\\pm 1\\rangle. \\end{align} $$\nI\u0026rsquo;ll again emphasize we are working in the purely Zeeman basis for two electrons and one nucleus. Each basis state is of the form $|m_a, m_b, m_I\\rangle$, where each $m_i \\in \\{\\uparrow, \\downarrow \\}$.\nLet\u0026rsquo;s go step by step to evaluate $\\hat{H}_{HF} |m_a, m_b, m_I\\rangle$. From Equation (5), we find\n$$ \\begin{align*} \\hat{S}_+ |\\uparrow\\rangle = 0 \\qquad \\hat{S}_+|\\downarrow\\rangle = \\hbar|\\uparrow\\rangle \\\\ \\hat{S}_- |\\downarrow\\rangle = 0 \\qquad \\hat{S}_-|\\uparrow\\rangle = \\hbar|\\downarrow\\rangle \\\\ \\hat{S}_z|m\\rangle = m\\hbar |m\\rangle, \\quad m \\in \\{\\uparrow, \\downarrow \\}. \\end{align*} $$\nLikewise for the nuclear state, where we use $m\\in \\{\\Uparrow, \\Downarrow\\}$,\n$$ \\begin{align*} \\hat{I}_+ |\\Uparrow\\rangle = 0 \\qquad \\hat{I}_+|\\Downarrow\\rangle = \\hbar|\\Uparrow\\rangle \\\\ \\hat{I}_- |\\Downarrow\\rangle = 0 \\qquad \\hat{I}_-|\\Uparrow\\rangle = \\hbar|\\Downarrow\\rangle \\\\ \\hat{I}_z|m_I\\rangle = m_I\\hbar |m_I\\rangle, \\quad m_I \\in \\{\\Uparrow, \\Downarrow \\}. \\end{align*}. $$\nThere are five nontrivial (that don\u0026rsquo;t end up = 0) operator products. One electron $\\{\\uparrow, \\downarrow\\}$ represents both electrons $a, b$ (i.e. $|\\downarrow, \\Downarrow\\rangle = |\\downarrow, \\downarrow, \\Downarrow\\rangle$).\n$$ \\begin{align*} \u0026amp;\\text{product} \u0026amp;\u0026amp;\\text{nonzero for} \u0026amp;\u0026amp;\u0026amp;\\text{result} \\\\ \u0026amp;\\hat{S}_{k+} \\hat{I}_+ \u0026amp;\u0026amp; |\\downarrow, \\Downarrow\\rangle \u0026amp;\u0026amp;\u0026amp; \\hbar^2|\\uparrow, \\Uparrow\\rangle \\\\ \u0026amp;\\hat{S}_{k+} \\hat{I}_- \u0026amp;\u0026amp; |\\downarrow, \\Uparrow\\rangle \u0026amp;\u0026amp;\u0026amp; \\hbar^2 |\\uparrow, \\Downarrow\\rangle \\\\ \u0026amp;\\hat{S}_{k-} \\hat{I}_+ \u0026amp;\u0026amp; |\\uparrow, \\Downarrow\\rangle \u0026amp;\u0026amp;\u0026amp; \\hbar^2 |\\downarrow, \\Uparrow \\rangle \\\\ \u0026amp;\\hat{S}_{k-} \\hat{I}_- \u0026amp;\u0026amp; |\\uparrow, \\Uparrow\\rangle \u0026amp;\u0026amp;\u0026amp; \\hbar^2 |\\downarrow, \\Downarrow \\rangle \\\\ \u0026amp;\\hat{S}_{kz} \\hat{I}_z \u0026amp;\u0026amp;\\text{any} \u0026amp;\u0026amp;\u0026amp; \\hbar^2 m_{a, b} m_I |m_{a, b}, m_I \\rangle. \\end{align*} $$\nThe last term contributes to diagonal terms only. It leaves the state $|m_{a, b}, m_I\\rangle$ constant; adds no mixing.\nWe can construct the Hyperfine Hamiltonian in the Zeeman basis using this table.\n$$ \\begin{align*} \\hat{H}_{HF} |m_a, m_b, m_I\\rangle \u0026amp;= \\sum_{k\\in\\{a, b\\}} \\frac{\\hbar^2}{4} (A_{kx} - A_{ky}) \\hat{S}_{k+} \\hat{I}_+ \\delta_{m_k, m_I} |-m_k, \\quad, -m_I\\rangle \\\\ \u0026amp;\\qquad\\;\\; +\\frac{\\hbar^2}{4} (A_{kx} - A_{ky}) \\hat{S}_{k+} \\hat{I}_{-} \\delta_{m_k, -m_I} |-m_k,\\quad, -m_I\\rangle \\\\ \u0026amp;\\qquad\\;\\; + A_{kz} \\hbar^2 m_k m_I |m_k, \\quad, m_I\\rangle. \\end{align*} $$\nA blank $m_b$ means it can be either $\\uparrow$ or $\\downarrow$. Expanding the summation,\n$$ \\begin{align*} \\hat{H}_{HF} |m_a, m_b, m_I \\rangle \u0026amp;= \\hbar^2 (A_{az} m_a m_I + A_{bz}m_b m_I ) |m_a, m_b, m_I\\rangle \\\\ \u0026amp;+ \\frac{\\hbar^2}{4} [ (A_{ax} - A_{ay}) \\delta_{m_a, m_I} + (A_{ax} + A_{ay}) \\delta_{m_a, -m_I}] |-m_a, m_b, -m_I\\rangle \\\\ \u0026amp;+ \\frac{\\hbar^2}{4} [ (A_{bx} - A_{by}) \\delta_{m_b, m_I} + (A_{bx} + A_{by}) \\delta_{m_b, -m_I} ] |m_a, -m_b, m_I\\rangle, \\end{align*} $$\nwhere $m_a, m_b, m_I$ all run over $\\{\\uparrow, \\downarrow\\}$. The equation above defines $\\hat{H}_{HF}$ for an 8 Zeeman-basis two-electron + one-nucleus system. Evolving it to our two-electron + two-nuclei system in silicon carbide is straightforward.\nTwo electrons and two nuclei# For two electrons $k\\in\\{a, b\\}$ and two nuclei $p \\in \\{1, 2\\}$,\n$$ \\hat{H}_{HF} = \\sum_{k\\in\\{a, b\\}} \\sum_{p=1}^2 ( A_{kpx} S_{kx} I_{px} + A_{kpy} S_{ky} I_{py} + A_{kpz} S_{kz} I_{pz}). $$\nFollowing the same procedure as the one-nucleus system, we get the full Hyperfine Hamiltonian action on $|m_a, m_b, m_{I_1}, m_{I_2} \\rangle$.\n$$ \\boxed{ \\begin{align*} \\hat{H}_{HF} \u0026amp;|m_a, m_b, m_{I_1}, m_{I_2} \\rangle \\\\ \u0026amp;= \\hbar^2 \\sum_{p=1}^{2} (A_{apz} m_a m_{I_p} + A_{bpz} m_b m_{I_p}) |m_a, m_b, m_{I_1}, m_{I_2}\\rangle \\\\ \u0026amp;+ \\frac{\\hbar^2}{4} [ (A_{a1x} - A_{a1y}) \\delta_{m_a, m_{I_1}} + (A_{a1x} + A_{a1y}) \\delta_{m_a, -m_{I_1}} ] |-m_a, m_b, -m_{I_1}, m_{I_2}\\rangle \\\\ \u0026amp;+ \\frac{\\hbar^2}{4} [ (A_{a2x} - A_{a2y}) \\delta_{m_a, m_{I_2}} + (A_{a2x} + A_{a2y}) \\delta_{m_a, -m_{I_2}} ] |-m_a, m_b, m_{I_1}, -m_{I_2}\\rangle \\\\ \u0026amp;+ \\frac{\\hbar^2}{4} [ (A_{b1x} - A_{b1y}) \\delta_{m_b, m_{I_1}} + (A_{b1x} + A_{b1y}) \\delta_{m_b, -m_{I_1}} ] |m_a, -m_b, -m_{I_1}, m_{I_2}\\rangle \\\\ \u0026amp;+ \\frac{\\hbar^2}{4} [ (A_{b2x} - A_{b2y}) \\delta_{m_b, m_{I_2}} + (A_{b2x} + A_{b2y}) \\delta_{m_b, -m_{I_2}} ] |m_a, -m_b, m_{I_1}, -m_{I_2}\\rangle. \\end{align*} } $$\nHowever, we must remember this is the Zeeman $\\uparrow, \\downarrow$ basis. We need to convert it to the same coupled basis we used for defining the Zeeman Hamiltonian.\nIn Quantum mechanics, you learn about Clebsch-Gordan coefficients. These are coefficients that describe how to map the Zeeman $|\\uparrow, \\downarrow\\rangle$ basis to the coupled $|s, m\\rangle$ basis. We make a matrix transformation out of these coefficients.\nFor mapping the coupled basis to the Zeeman basis, we have\n$$ \\begin{pmatrix} |\\uparrow \\uparrow \\rangle \\\\ |\\uparrow \\downarrow \\rangle \\\\ |\\downarrow \\uparrow \\rangle \\\\ |\\downarrow \\downarrow \\rangle \\end{pmatrix} = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{2}} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix} |11\\rangle \\\\ |10\\rangle \\\\ |00\\rangle \\\\ |1-1\\rangle \\end{pmatrix}. $$\nTherefore, we cleverly define $W$:\n$$ \\begin{align} W = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{\\sqrt{2}} \u0026amp; -\\frac{1}{\\sqrt{2}} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\otimes \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}, \\end{align} $$\nwhere we $\\otimes \\mathbb{I}_4$ because we want to leave the nuclear $|m_{I_1}, m_{I_2}\\rangle$ in the Zeeman basis and only couple the electrons into $|s, m\\rangle$. Then, once we have the full 16 $\\times$ 16 Hyperfine Hamiltonian in the Zeeman Basis, we can directly apply the transformation\n$$ \\begin{align} \\hat{H}_{HF_{\\text{coupled}}} = W \\left(\\hat{H}_{HF_{\\text{Zeeman}}}\\right) W^\\dagger \\end{align} $$\nto get the final Hyperfine Hamiltonian in the coupled basis. We have to ensure that after this transformation, the ordering is identical to the ordering of the Zeeman Hamiltonian.\nImplementation# We first build the 16 Zeeman basis states in correct order. Electron states run slow. Nuclear states run fast.\n/// builds correctly ordered zeeman basis def _generate_zeeman_basis(self): half = sp.Rational(1, 2) return [ (m_a, m_b, m_I1, m_I2) for m_a in (half, -half) for m_b in (half, -half) for m_I1 in (half, -half) for m_I2 in (half, -half) ] // len 16 The trickiest part is emulating the boxed equation above. To begin, we define the following helper functions for working with delta functions.\n/// if spins are parallel def _delta_parallel(self, m_e, m_I): return m_e == m_I /// if spins are antiparallel def _delta_antiparallel(self, m_e, m_I): return m_e == -m_I Next, we have to write a function that takes in a $|m_a, m_b, m_{I_1}, m_{I_2}\\rangle$ and outputs the resulting coefficient in front from performing $\\hat{H}_{HF} |m_a, m_b, m_{I_1}, m_{I_2}\\rangle$.\nThe coefficients are of the form\n$$ \\frac{\\hbar^2}{4}\\left[(A_{i1x} - A_{i1y}) \\delta_{m_i, m_{I_j}} + (A_{i1x} + A_{i1y})\\delta_{m_i, -m_{I_j}} \\right], $$\nwhere $i \\in \\{a, b \\}$ and $j \\in \\{1, 2\\}$. We\u0026rsquo;ll do this by generating a dict[new_ket, coeff] that associates each key new_ket with its coeff in front step by step. In code we set $\\hbar = 1$ to simplify. It\u0026rsquo;ll get included later.\nHyperfine $A$ constants.# These are unknown constants that are present in $\\hat{H}_{HF}$. We generate another dict[key, sp.Symbol]. where the sp.Symbols are the actual constants. We\u0026rsquo;ll access them via their key\nself.A = {} for e in ('a', 'b'): for n in ('1', '2'): for comp in ('x', 'y', 'z'): key = f'A{e}{n}{comp}' self.A[key.upper()] = sp.symbols(key) Diagonal $\\hat{S}_z\\hat{I}_z$ terms.# The first term in the boxed $\\hat{H}_{HF}$ expression is simplest. It leaves $|m_a, m_b, m_{I_1}, m_{I_2}\\rangle$ as an eigenstate. The coefficients in front are just products of $A m_{a/b} m_{I_{1, 2}}$. We\u0026rsquo;ll attack these first.\ndef _action_on_ket(self, ket): m_a, m_b, m_I1, m_I2 = ket // {new_ket: coeff} psi = {} // diagonal S_z I_z term // A * m_e * m_I diag = ( self.A['AA1Z'] * m_a * m_I1 + // A_{a1z} self.A['AB1Z'] * m_b * m_I1 + // A_{b1z} self.A['AA2Z'] * m_a * m_I2 + // A_{a2z} self.A['AB2Z'] * m_b * m_I2 // A_{b2z} ) psi[ket] = diag // more to come Off-Diagonal $\\delta$ terms# These coefficients are more difficult. They involve delta functions that either do nothing or collapse states to 0 depending on spins $m_i$ and $m_{I_j}$ being parallel or antiparallel. This will be where we use our _delta_parallel and _delta_antiparallel helpers.\n/// off-diagonal flip-flop /// S_x I_x + S_y I_y -\u0026gt; (A_x - A_y)/4 def add_flip(m_e, m_I, x_key, y_key, new_ket): if self._delta_parallel(m_e, m_I): coeff = (self.A[x_key] - self.A[y_key]) / 4 psi[new_ket] = coeff elif self._delta_antiparallel(m_e, m_I): coeff = (self.A[x_key] + self.A[y_key]) / 4 psi[new_ket] = coeff // --- final states we know add_flip(m_a, m_I1, 'AA1X', 'AA1Y', (-m_a, m_b, -m_I1, m_I2)) add_flip(m_a, m_I2, 'AA2X', 'AA2Y', (-m_a, m_b, m_I1, -m_I2)) add_flip(m_b, m_I1, 'AB1X', 'AB1Y', ( m_a, -m_b, -m_I1, m_I2)) add_flip(m_b, m_I2, 'AB2X', 'AB2Y', ( m_a, -m_b, m_I1, -m_I2)) return psi Now, evaluating $\\hat{H}_{HF}$ on every Zeeman state gives us the full Hyperfine Hamiltonian in the Zeeman basis. We just iterate through and build. SymPy syntax isn\u0026rsquo;t important.\n/// 16x16 hyperfine hamiltonian /// zeeman Basis def _build_zeeman_matrix(self): self.basis_ze = self._generate_zeeman_basis() self.index_ze = { ket: i for i, ket in enumerate(self.basis_ze) } size = len(self.basis_ze) // initialize 16x16 matrix H = sp.MutableDenseMatrix(size, size, lambda *_: 0) // iterate through zeeman states for j, ket in enumerate(self.basis_ze): action = self._action_on_ket(ket) // input coefficients into matrix for new_ket, coeff in action.items(): i = self.index_ze[new_ket] H[i, j] = coeff return H.as_immutable() Now all that is left is to apply the Clebsch-Gordan $W$ transformation in Equation (7). Let\u0026rsquo;s first build $W$ from Equation (6). It\u0026rsquo;s a tensor product of a $4\\times 4$ Clebsch-Gordan matrix with the $4\\times 4$ identity.\n/// builds the unitary Clebsch-Gordan transformation W def _build_cg_unitary(self): half = sp.sqrt(sp.Rational(1, 2)) U = sp.Matrix([ // 4x4 CG [1, 0, 0, 0], [0, half, half, 0], [0, half, -half, 0], [0, 0, 0, 1] ]) I4 = sp.eye(4) // 4x4 identity return sp.kronecker_product(U, I4) // W Finally! We can calculate $\\hat{H}_{HF}$ in our ordered coupled-basis.\n// zeeman basis self.H_ze = self._build_zeeman_matrix() // clebsch gordan W self.W = self._build_cg_unitary() // final hyperfine hamiltonian // coupled basis self.H_HF = sp.simplify(self.W * self.H_ze * self.W.H) Zero-Field Splitting# In Part 2 we derived $\\hat{H}_{ZFS}$\u0026rsquo;s action on a general $|s, m\\rangle$.\n$$ \\begin{align} \\hat{H}_{ZFS}|s, m\\rangle \u0026amp;= Dm^2 |s, m\\rangle - \\frac{D}{3}(s(s+1))|s, m\\rangle \\\\ \u0026amp;+ \\frac{E}{2} (s(s+1) - m(m+1))|s, m+2\\rangle \\\\ \u0026amp;+ \\frac{E}{2} (s(s+1) - m(m-1))|s, m-2\\rangle. \\end{align}. $$\nThe last two terms give the forbidden $m \\pm 2$ transitions that create the half-field response (to be described later). This Hamiltonian acts on the electrons only. It\u0026rsquo;s a dipole-dipole interaction. Nuclear terms are left alone. That makes things quite easy.\nImplementation# We directly build $\\hat{H}_{ZFS}$ while maintaining the ordered basis. From the Zeeman Implementation, we have\n// electron |s, m\u0026gt; pairs electron_pairs = [ (1, +1), (1, 0), (0, 0), (1, -1), ] We\u0026rsquo;ll just use that again to maintain the basis. The first term, Equation (8), give the diagonal elements ($|s, m\\rangle$ are the eigenstates). Equation (9-10) give the off-diagonal elements that mix $m \\rightarrow m\\pm 2$.\n// zfs constants D = sp.symbols('D') E = sp.symbols('E') // 4x4 // zfs on coupled electrons H_elec = sp.zeros(4) for i, (s, m) in enumerate(electron_pairs): // diagonal term // D * m^2 - (D/3) * s(s+1) H_elec[i, i] = D*m**2 - (D/3)*s*(s + 1) // off-diagonal term // coupling m -\u0026gt; m +/- 2 for dm, expr in [ (2, E/2*(s*(s + 1) - m*(m + 1))), (-2, E/2*(s*(s + 1) - m*(m - 1))) ]: // m +/- 2 final_m = m + dm if (s, final_m) in electron_pairs: j = electron_pairs.index((s, final_m)) H_elec[i, j] = expr H_elec[j, i] = expr // 4x4 // nuclei identity I_nuc = sp.eye(4) // 16x16 zfs hamiltonian H_ZFS = sp.kronecker_product(H_elec, I_nuc) We take the tensor product $\\hat{H}_{ZFS_{\\text{electron}}} \\otimes \\mathbb{I}_4$ again to lift the electronic $4\\times 4$ Hamiltonian to a $16\\times 16$ one in our Hilbert space. This gives $\\hat{H}_{ZFS}$ in the coupled-basis.\nExchange Interaction# From Part 3, we derived\n$$ \\hat{H}_{EX} = -J \\hat{S}_a \\cdot \\hat{S}_b, $$\nLet\u0026rsquo;s take $\\hat{S} = \\hat{S}_a + \\hat{S}_b$. Then\n$$ \\hat{S}^2 = \\hat{S}_a^2 + \\hat{S}_b^2 + 2 \\hat{S}_a \\cdot \\hat{S}_b. $$\nRearranging to calculate $\\hat{S}_a \\cdot \\hat{S}_b$, we find that\n$$ \\hat{S}_a \\cdot \\hat{S}_b = \\frac{1}{2}(\\hat{S}^2 - \\hat{S}_a^2 - \\hat{S}_b^2). $$\nActing on the coupled electron basis $|s, m\\rangle$, the three terms in the parenthesis satisfy\n$$ \\begin{align*} \\hat{S}^2 |s, m\\rangle \u0026amp;= \\hbar^2 s(s+1)|s, m\\rangle, \\ \\hat{S}_a^2 |s, m\\rangle \u0026amp;= \\frac{3\\hbar^2}{4}|s, m\\rangle, \\ \\hat{S}_b^2 |s, m\\rangle \u0026amp;= \\frac{3\\hbar^2}{4}|s, m\\rangle. \\end{align*} $$\nHence,\n$$ \\hat{S}_a \\cdot \\hat{S}_b |s, m\\rangle = \\frac{\\hbar^2}{2}\\left[ s(s+1) - \\frac{3}{2} \\right] |s, m\\rangle. $$\n$\\hat{H}_{EX}$ immediately follows:\n$$ \\hat{H}_{EX} |s, m\\rangle = -J \\frac{\\hbar^2}{2} \\left[ s(s+1) - \\frac{3}{2}\\right] |s, m\\rangle. $$\nWe\u0026rsquo;ll also program this with $\\hbar = 1$. $\\hat{H}_{EX}$ already acts on the coupled-basis (nuclear $|m_I\\rangle$ are left alone). Additionally, all states are eigenstates; $\\hat{H}_{EX}$ is diagonal. Programming this is the easiest.\n// exchange constant J = sp.symbols('J') // 4x4 H_ex_elec = sp.zeros(4) for i, (s, m) in enumerate(electron_pairs): H_ex_elec[i, i] = -J*(s*(s + 1) - 1.5) / 2 // 4x4 // nuclear identity I_nuc = sp.eye(4) // 16x16 exchange hamiltonian H_EX = sp.kronecker_product(H_ex_elec, I_nuc) Again, we take the tensor product of the 4$\\times$ 4 electron Hamiltonian with the 4$\\times$ 4 identity to yield the full 16$\\times$ 16 $\\hat{H}_{EX}$ in our Hilbert space.\nFull Spin# The Spin Hamiltonian $\\mathscr{H}$ is just\nH_SPIN = H_Z + H_HF + H_ZFS + H_EX Basis# The coupled-basis is shown below. Indices 0-15 indicate top-to-bottom and left-to-right row/column basis states in $\\mathscr{H}$\u0026rsquo;s matrix representation.\n$$ \\begin{align*} 0 \u0026amp;\u0026amp; \\qquad |1,1\\rangle \\otimes |+\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 1 \u0026amp;\u0026amp; \\qquad |1,1\\rangle \\otimes |+\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\\\ 2 \u0026amp;\u0026amp; \\qquad |1,1\\rangle \\otimes |-\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 3 \u0026amp;\u0026amp; \\qquad |1,1\\rangle \\otimes |-\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\\\ 4 \u0026amp;\u0026amp; \\qquad |1,0\\rangle \\otimes |+\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 5 \u0026amp;\u0026amp; \\qquad |1,0\\rangle \\otimes |+\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\\\ 6 \u0026amp;\u0026amp; \\qquad |1,0\\rangle \\otimes |-\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 7 \u0026amp;\u0026amp; \\qquad |1,0\\rangle \\otimes |-\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\\\ 8 \u0026amp;\u0026amp; \\qquad |0,0\\rangle \\otimes |+\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 9 \u0026amp;\u0026amp; \\qquad |0,0\\rangle \\otimes |+\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\\\ 10 \u0026amp;\u0026amp; \\qquad |0,0\\rangle \\otimes |-\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 11 \u0026amp;\u0026amp; \\qquad |0,0\\rangle \\otimes |-\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\\\ 12 \u0026amp;\u0026amp; \\qquad |1,-1\\rangle \\otimes |+\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 13 \u0026amp;\u0026amp; \\qquad |1,-1\\rangle \\otimes |+\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\\\ 14 \u0026amp;\u0026amp; \\qquad |1,-1\\rangle \\otimes |-\\tfrac{1}{2}, +\\tfrac{1}{2}\\rangle \\\\ 15 \u0026amp;\u0026amp; \\qquad |1,-1\\rangle \\otimes |-\\tfrac{1}{2}, -\\tfrac{1}{2}\\rangle \\end{align*}. $$\n$|+\\frac{1}{2}\\rangle = |\\Uparrow\\rangle$ and $|-\\frac{1}{2}\\rangle = |\\Downarrow\\rangle$ (nuclei-only).\nNow we have a Spin Hamiltonian. However, it has many unknown constants.\n$$ \\begin{align*} \u0026amp;g_e \u0026amp;\u0026amp; \\quad \\text{electron $g$-factor} \\\\ \u0026amp;g_{n_1}, \\, g_{n_2} \u0026amp;\u0026amp; \\quad \\text{nuclear $g$-factor (Si, C)} \\\\ \u0026amp;\\mu_B \u0026amp;\u0026amp;\\quad \\text{Bohr magneton} \\\\ \u0026amp;\\mu_N \u0026amp;\u0026amp;\\quad \\text{Nuclear magneton} \\\\ \u0026amp;B_0 \u0026amp;\u0026amp;\\quad \\text{external magnetic field} \\\\ \u0026amp;A_{ijx}, \\, A_{ijy}, \\, A_{ijz} \u0026amp;\u0026amp;\\quad \\text{hyperfine tensor} \\\\ \u0026amp;D \u0026amp;\u0026amp;\\quad\\text{axial zero-field} \\\\ \u0026amp;E \u0026amp;\u0026amp;\\quad\\text{transverse zero-field} \\\\ \u0026amp;J \u0026amp;\u0026amp;\\quad \\text{exchange coupling} \\end{align*} $$\nOur next challenge is to perform eigenenergy simulations. I want a plot like in Part 1 but for every combination of every sub-Hamiltonian in $\\mathscr{H}$, and for all 16 coupled-basis states.\nThis may seem ambitious, but it\u0026rsquo;s very straightforward. We already have matrices for all the Hamiltonians. We just need to substitute values for all the constants above and sweep over the $B_0$ field parameter in $\\hat{H}_Z$. Then we\u0026rsquo;ll use LAPACK to calculate the energy eigenvalues at each $B_0$ field point. Consequently we\u0026rsquo;ll have our plots for $B_0$ vs. energy.\n← Previous "},{"id":6,"href":"/posts/nasa_derive_3/","title":"The Spin Hamiltonian","section":"posts","content":" This post is math heavy. I\u0026rsquo;ll try to walk through it all elegantly though. The important equations are boxed if you\u0026rsquo;d like to skip through. On mobile some equations may go overfull.\nIn Part 1 we derived the Zeeman Hamiltonian and the Hyperfine Hamiltonian. In Part 2 we derived the Zero-Field Splitting Hamiltonian analogously. The goal of this post is to derive the last sub-Hamiltonian for the Exchange Interaction and consequently derive the complete Spin Hamiltonian $\\mathscr{H}$.\nI use a fancy $\\mathscr{H}$ to represent the Spin Hamiltonian. Sub-Hamiltonians are normal $H$\u0026rsquo;s but with hats (e.g. $\\hat{H}_Z$).\nThe Exchange Interaction# Quantum mechanics says electrons are indistinguishable fermions. Their combined spatial + spin wave function must be antisymmetric. This symmetry links neighboring electrons spins and positions. $\\hat{H}_{EX}$ is another term that shifts energy levels.\nConsider two electrons in orbitals $\\phi_a (\\vec{r}), \\phi_b(\\vec{r})$. The two-electron Hamiltonian is\n$$ H = \\sum_{i=1}^2 \\left[\\underbrace{-\\frac{\\hbar^2}{2m} \\nabla_i^2}_{\\text{kinetic energy}} + \\underbrace{V(\\vec{r}_i)}_{\\text{potential energy}}\\right] + \\underbrace{\\frac{e^2}{4\\pi\\epsilon_0} |\\vec{r}_1 - \\vec{r}_2|}_{\\text{Coulomb repulsion}}. $$\nHere, $V(\\vec{r})$ is the single-particle potential. Because electrons are fermions, their state must be antisymmetric under spatial and spin exchange. We factorize into spatial $\\Psi(\\vec{r}_1, \\vec{r}_2)$ and spin $\\chi(s_1, s_2)$ parts:\n$$ \\Psi_{\\text{tot}} (1, 2) = \\Psi(\\vec{r}_1, \\vec{r}_2)\\chi(s_1, s_2), $$\nwhere antisymmetry requires\n$$ \\Psi_{\\text{tot}}(2, 1) = -\\Psi_{\\text{tot}}(1, 2). $$\nIf spatial $\\Psi$ is symmetric, then the spin state $\\chi$ must be antisymmetric (a singlet). If $\\Psi$ is antisymmetric, $\\chi$ must be symmetric (a triplet).\nWe build orthonormal spatial orbitals $\\phi_a, \\phi_b$. Let\u0026rsquo;s choose\n$$ \\begin{align*} \\Psi_S(\\vec{r}_1, \\vec{r}_2) \u0026amp;= \\frac{1}{\\sqrt{2}}[\\phi_a(1) \\phi_b(2) + \\phi_b(1)\\phi_a(2)] \\\\ \\Psi_A(\\vec{r}_1, \\vec{r}_2) \u0026amp;= \\frac{1}{\\sqrt{2}}[\\phi_a(1)\\phi_b(2) - \\phi_b(1)\\phi_a(2)]. \\end{align*} $$\nThe corresponding spin states are\n$$ \\begin{align*} \\chi_S \u0026amp;= \\frac{1}{\\sqrt{2}}(|\\uparrow \\downarrow\\rangle - |\\downarrow \\uparrow\\rangle) \\quad \\text{(singlet)}, \\\\ \\chi_A \u0026amp;= \\frac{1}{\\sqrt{2}}(|\\uparrow \\downarrow \\rangle + |\\downarrow \\uparrow\\rangle) \\quad \\text{(triplet)}. \\end{align*} $$\nWe can diagonalize $H$ into the two-dimensional subspace spanned by $\\phi_a\\phi_b$. Both electrons share the same one-particle energy,\n$$ E_0 = \\langle \\phi_a | \\hat{h} | \\phi_a\\rangle + \\langle \\phi_b|\\hat{h}|\\phi_b\\rangle, \\quad \\hat{h} = -\\frac{\\hbar^2}{2m}\\nabla^2 + V(\\vec{r}). $$\nTherefore, any energy splitting arises entirely from the Coulomb term.\nEnergy Splitting# Let\u0026rsquo;s define the \u0026ldquo;direct\u0026rdquo; and \u0026ldquo;exchange\u0026rdquo; integrals:\n$$ \\begin{align*} K \u0026amp;\\equiv \\int \\int d^3r_1 d^3r_2 \\, |\\phi_a (1)|^2 \\frac{e^2}{4\\pi\\epsilon_0 r_{12}} |\\phi_b(2)|^2, \\\\ J \u0026amp;\\equiv \\int \\int d^3r_1 d^3r_2 \\phi_a^* (1) \\phi_b (1) \\frac{e^2}{4\\pi\\epsilon_0r_{12}} \\phi_b^* (2) \\phi_a (2). \\end{align*} $$\nBy magic,\n$$ \\begin{align} \\langle \\Psi_S | H | \\Psi_S \\rangle = E_0 + K + J, \\\\ \\langle \\Psi_A | H | \\Psi_A \\rangle = E_0 + K - J. \\end{align} $$\nThus the singlet sits at energy $E_S = E_0 + K + J$, while each triplet has $E_T = E_0 + K - J$. Their energy difference/splitting is\n$$ \\begin{align} \\Delta E \\equiv E_S - E_T = 2J. \\end{align} $$\nThe energy splitting due to the Zero Field $\\hat{H}_{ZFS}$ is analogously also $2D$. We now wish to replace this two-level spatial + spin system with a pure spin Hamiltonian that reproduces the same energy shift between singlet and triplet.\nExchange Operator# Recall that the operator exchanging two spins is\n$$ \\hat{P}_{12}\\chi(1, 2) = \\chi(2, 1). $$\nIts eigenvalues are +1 on the symmetric (triplet) subspace and -1 on the antisymmetric (singlet) subspace. Define the total spin operator and its square\n$$ \\begin{align*} \\hat{\\vec{S}}_{\\text{tot}} \u0026amp;= \\hat{\\vec{S}}_1 + \\hat{\\vec{S}}_2, \\\\ \\hat{S}_{\\text{tot}}^2 \u0026amp;= \\hat{S}_1^2 + \\hat{S}_2^2 + 2\\hat{\\vec{S}}_1 \\cdot \\hat{\\vec{S}}_2. \\end{align*} $$\nfor spin-1/2 particles, $\\hat{S}_1^2 = \\hat{S}_2^2 = \\hat{S}_x^2 + \\hat{S}_y^2 + \\hat{S}_z^2 = \\frac{3\\hbar^2}{4}\\mathbb{I}$. Hence,\n$$ \\hat{S}_{\\text{tot}}^2 = \\frac{3\\hbar^2}{2}\\mathbb{I} + 2\\hat{\\vec{S}}_1 \\cdot \\hat{\\vec{S}}_2. $$\nNow for both triplet ($S_{\\text{tot}} = 1$) and singlet ($S_{\\text{tot}} = 0$) states,\n$$ \\begin{align} \\hat{S}_{\\text{tot}}^2 |10\\rangle \u0026amp;= S(S+1)\\hbar^2 |10\\rangle = 2\\hbar^2 |10\\rangle \\\\ \\hat{S}_{\\text{tot}}^2 |00\\rangle \u0026amp;= S(S+1)\\hbar^2 |00\\rangle = 0 \\end{align} $$\nBut we need an exchange operator that satisfies\n$$ \\begin{align} \\hat{P}_{12}|10\\rangle \u0026amp;= +|10\\rangle \\\\ \\hat{P}_{12}|00\\rangle \u0026amp;= -|00\\rangle. \\end{align} $$\nWe define an ansatz $\\hat{P}_{12} = a \\hat{S}_{\\text{tot}}^2 + b \\mathbb{I}$. Plugging Equations (4) and (5) into (6) and (7) gives\n$$ \\begin{align*} \\hat{P}_{12} |10\\rangle \u0026amp;= a(2) + b |10\\rangle = +1 |00\\rangle \\\\ \\hat{P}_{12} |00\\rangle \u0026amp;= a(0) + b |00\\rangle = -1 |00\\rangle. \\end{align*} $$\nSolving for a and b gives $a = 1$ and $b = -1$. So we get the exchange operator:\n$$ \\hat{P}_{12} = 1 \\cdot \\hat{S}_{\\text{tot}}^2 - 1 \\cdot \\mathbb{I} = \\hat{S}_{\\text{tot}}^2 - \\mathbb{I}. $$\nThe final expression for the exchange operator is\n$$ \\hat{P}_\\text{12} = \\frac{1}{2} \\left(1 + 4 \\hat{\\vec{S}}_1 \\cdot \\hat{\\vec{S}}_2 \\right). $$\nIts eigenvalues are correctly -1 on the singlet and +1 on the triplet manifold. Hence, an operator proportional to $\\hat{P}_{12}$ will split singlet/triplet.\nProportionality Coefficient# Define the proportionality coefficient $J_{ex}$ such that\n$$ \\hat{H}_{EX} = J_{ex} \\hat{P}_{12} = \\frac{J_{ex}}{2} \\left(1 + 4\\hat{\\vec{S}}_1 \\cdot \\hat{\\vec{S}}_2\\right). $$\nActing on the singlet and triplet manifolds:\n$$ \\begin{align} \\hat{H}_{EX} \\chi_S \u0026amp;= J_{ex}(-1)\\chi_S \\\\ \\hat{H}_{EX} \\chi_T \u0026amp;= J_{ex}(+1)\\chi_T. \\end{align} $$\nTo reproduce the $\\Delta E = 2J$ energy splitting from Equation (2) demands $J_{ex} = -J$. Then\n$$ E_{S} = -J_{ex} = +J \\qquad E_{T} = +J_{ex} = -J; $$ $$ \\Delta E = 2J, $$\nas desired. We set $J$ to be the exchange coupling matrix proportional to the degree of overlap between wave functions. The Exchange Hamiltonian can finally be expressed as\n$$ \\hat{H}_{EX} = \\hat{\\vec{S}}_a \\cdot J \\cdot \\hat{\\vec{S}}_b. $$\nAssuming the exchange coupling is isotropic, we get\n$$ \\boxed{ \\hat{H}_{EX} = -J \\hat{\\vec{S}}_a \\cdot \\hat{\\vec{S}}_b. } $$\nThis is the final component of the 16 $\\times$ 16 Spin Hamiltonian. Together, the Zeeman, Hyperfine, ZFS, and Exchange Hamiltonians can describe the mechanics of spin-dependent recombination.\nBasis States# For a two-electron (carrier + defect) and two-nuclei (silicon and carbon) spin system, there are 16 orthonormal basis states. Our Spin Hamiltonian is a matrix of shape $16 \\times 16$.\nTo make future calculations easier, we\u0026rsquo;ll define our basis states with the electrons coupled ($|s, m\\rangle$) and the nuclei in their Zeeman $\\{\\uparrow, \\downarrow \\} \\equiv \\{ +\\frac{1}{2}, -\\frac{1}{2} \\}$ basis.\nTriplet $|1, +1\\rangle$ manifold# $$ \\begin{cases} |1,1\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |1,1\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\\\ |1,1\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |1,1\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\end{cases} $$\nTriplet $|1, 0\\rangle$ manifold# $$ \\begin{cases} |1,0\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |1,0\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\\\ |1,0\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |1,0\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\end{cases} $$\nSinglet $|0, 0\\rangle$ manifold# $$ \\begin{cases} |0,0\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |0,0\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\\\ |0,0\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |0,0\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\end{cases} $$\nTriplet $|1, -1\\rangle$ manifold# $$ \\begin{cases} |1,-1\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |1,-1\\rangle \\otimes |+\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\\\ |1,-1\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |+\\tfrac{1}{2}\\rangle_2 \\\\ |1,-1\\rangle \\otimes |-\\tfrac{1}{2}\\rangle_1 \\otimes |-\\tfrac{1}{2}\\rangle_2 \\end{cases} $$\nThe first $|s, m\\rangle$ states represent the two electrons in their coupled basis. The second $|\\pm\\frac{1}{2}\\rangle_1 \\otimes |\\pm \\frac{1}{2}\\rangle_2$ states represent the two nuclei in their Zeeman basis.\nNow we are ready to programmatically calculate the Spin Hamiltonian $\\mathscr{H}$. We\u0026rsquo;ll evaluate its effect on each basis state above. To do this, we program the Zeeman, Hyperfine, ZFS, and Exchange Hamiltonian\u0026rsquo;s effect on each basis state, and then sum them together.\nNext → ← Previous "},{"id":7,"href":"/posts/nasa_derive_2/","title":"Zero-Field Splitting","section":"posts","content":" This post is math heavy. I\u0026rsquo;ll try to walk through it all elegantly though. The important equations are boxed if you\u0026rsquo;d like to skip through. On mobile some equations may go overfull.\nIn Part 1 we derived the Zeeman Hamiltonian and the Hyperfine Hamiltonian. The goal of this post is to provide a concise derivation of the next Hamiltonian term for Zero-Field Splitting. The next post will do the last Exchange Interaction term and finally give a complete Spin Hamiltonian description.\nI use a fancy $\\mathscr{H}$ to represent the full Spin Hamiltonian. Sub-Hamiltonians are normal $H$\u0026rsquo;s but with hats (e.g. $\\hat{H}_Z$).\nZero-Field Splitting# The Zero Field Splitting (ZFS) comes from the magnetic field generated by one electron\u0026rsquo;s spin acting on another electron\u0026rsquo;s magnetic moment. Even when no external $B$ field is applied, this dipole-dipole interaction still shifts the spin energy levels. We can treat ZFS directly as an electron-electron analog of the hyperfine interaction.\nWe start immediately from Equation (6) in Part 1:\n$$ \\begin{align} \\vec{B}(\\vec{r}) = \\underbrace{\\frac{\\mu_0}{4\\pi r^3} \\left[ 3(\\vec{\\mu}_n \\cdot \\hat{r}) \\hat{r} - \\vec{\\mu}_n \\right]}_{\\text{dipolar term}} + \\underbrace{\\frac{2\\mu_0}{3}\\vec{\\mu}_n \\delta^3 (\\vec{r})}_{\\text{contact (isotropic) term}}. \\end{align} $$\nThe contact term, with the delta function $\\delta^3(\\vec{r})$ exists for a point-like nucleus. For Hyperfine, the nucleus sits at a point. When we take the curl of $\\vec{A}$, we pick up the singular term $\\frac{2\\mu_0}{3}\\vec{\\mu}_n \\delta^3(\\vec{r})$.\nHowever, for electron-electron interactions, electrons don\u0026rsquo;t coincide exactly. There is no $\\delta$-contact term. Only the $1/r^3$ dipolar piece survives from taking the curl. Hence, the magnetic field generated by an electron is\n$$ \\vec{B}_1(\\vec{r}) = \\frac{\\mu_0}{4\\pi r^3} [3(\\vec{\\mu}_1 \\cdot \\hat{r}) \\hat{r} - \\vec{\\mu}_1], \\quad \\hat{r} = \\frac{\\vec{r}}{r}. $$\n$\\vec{\\mu}_1$ is the magnetic moment of electron 1 and $\\vec{B}_1(\\vec{r})$ is the magnetic field generated by electron 1 at point $\\vec{r}$. From Equation (1) in Part 1, we can calculate the energy experienced by a second electron at $\\vec{r}$ from this field.\nA second electron has magnetic moment $\\vec{\\mu}_2$. So we get\n$$ \\begin{align} U = -\\vec{\\mu}_2 \\cdot \\vec{B}_1(\\vec{r}) = \\frac{\\mu_0}{4\\pi r^3} [ -\\vec{\\mu}_1 \\cdot \\vec{\\mu}_2 + 3 (\\vec{\\mu}_1 \\cdot \\hat{r})(\\vec{\\mu}_2 \\cdot \\hat{r}) ]. \\end{align} $$\nNow we can promote $U \\rightarrow \\hat{H}_{ZFS}$. Given\n$$ \\hat{\\vec{\\mu}}_i = -g_e \\mu_B \\frac{\\hat{\\vec{S}}_i}{\\hbar}, \\quad \\mu_B = \\frac{e\\hbar}{2m_e}. $$\nTherefore, replacing $\\vec{\\mu}_{1,2}$ with the quantum operators $\\hat{\\vec{\\mu}}_{1,2}$, the pre-factor in Equation (2) becomes\n$$ -\\frac{\\mu_0}{4\\pi} \\frac{(g_e\\mu_B)^2}{\\hbar^2}. $$\nAnd substituting yields\n$$ \\begin{align} \\hat{H}_{ZFS} \u0026amp;= -\\frac{\\mu_0}{4\\pi} \\frac{(g_e\\mu_B)^2}{\\hbar^2} \\frac{1}{r^3} \\left[-\\hat{\\vec{S}}_1 \\cdot \\hat{\\vec{S}}_2 + 3(\\hat{\\vec{S}}_1 \\cdot \\hat{r})(\\hat{\\vec{S}}_2 \\cdot \\hat{r})\\right] \\\\ \u0026amp;= \\frac{\\mu_0}{4\\pi} \\frac{(g_e \\mu_B)^2}{\\hbar^2} \\frac{1}{r^3} \\left[ \\hat{\\vec{S}}_1 \\cdot \\hat{\\vec{S}}_2 - 3(\\hat{\\vec{S}}_1 \\cdot \\hat{r})(\\hat{\\vec{S}}_2 \\cdot \\hat{r})\\right]. \\end{align} $$\nNow we simplify into an analogous form of the Hyperfine Hamiltonian. We have the equalities\n$$ \\begin{align} (\\vec{S}_1 \\cdot \\vec{S}_2) \u0026amp;= \\sum_{i \\in {x, y, z}} S_{1i} S_{2i} \\\\ (\\vec{S}_1 \\cdot \\hat{r}) (\\vec{S}_2 \\cdot \\hat{r}) \u0026amp;= \\left(\\sum_i S_{1i}\\hat{r}_i \\right) \\left( \\sum_j S_{2j}\\hat{r}_j\\right) \\\\ \u0026amp;= \\sum_{i, j} S_{1i} (\\hat{r}_i \\hat{r}_j) S_{2j} \\\\ \u0026amp;= \\sum_{i, j} S_{1i} \\frac{r_i r_j}{r^2} S_{2j}. \\end{align} $$\nPlugging this into Equation (4) gives\n$$ \\begin{align} \\hat{H}_{ZFS} \u0026amp;= \\frac{\\mu_0}{4\\pi} \\frac{(g_e \\mu_B)^2}{\\hbar^2} \\frac{1}{r^3} \\left( \\sum_i S_{1i}S_{2i} - 3 \\sum_{i, j} S_{1i} \\frac{r_i r_j}{r^2} S_{2j} \\right) \\\\ \u0026amp;= \\frac{\\mu_0}{4\\pi} \\frac{(g_e \\mu_B)^2}{\\hbar^2} \\left( \\sum_i \\frac{\\delta_{i,j}}{r^3} S_{1i}S_{2j} - 3\\sum_{i, j} \\frac{r_i r_j}{r^5} S_{1i}S_{2j} \\right) \\\\ \u0026amp;= \\sum_{i, j} S_{1i} \\left[ \\frac{\\mu_0}{4\\pi} \\frac{(g_e \\mu_B)^2}{\\hbar^2} \\left( \\frac{\\delta_{i, j}}{r^3}- \\frac{3r_ir_j}{r^5} \\right) \\right] S_{2j}. \\end{align} $$\nTo simplify, we define the tensor $D_{ij}$\n$$ \\begin{align} D_{ij}(\\vec{r}) = \\frac{\\mu_0}{4\\pi} \\frac{(g_e \\mu_B)^2}{\\hbar^2} \\left( \\frac{\\delta_{i,j}}{r^3} - \\frac{3r_i r_j}{r^5} \\right). \\end{align} $$\nThen we get a similar equation to the Hyperfine Hamiltonian:\n$$ \\begin{align} \\hat{H}_{ZFS} = \\sum_{i, j \\in \\{x, y, z\\}} S_{1i} \\, D_{ij}(\\vec{r}) \\, S_{2j} = \\hat{\\vec{S}} \\cdot D \\cdot \\hat{\\vec{S}}. \\end{align} $$\n$D$ and $E$# However, our work with ZFS isn\u0026rsquo;t done. Let\u0026rsquo;s diagonalize $D$ via the defect\u0026rsquo;s principal axis. We start by simplifying Equation (12). If we take the trace of the expression in the parenthesis, we find\n$$ \\begin{align*} \\sum_i \\left( \\frac{\\delta_{i,i}}{r^3} - \\frac{3r_i r_i}{r^5} \\right) \u0026amp;= \\sum_i \\frac{1}{r^3} - 3\\sum_i \\frac{r_i^2}{r^5} \\\\ \u0026amp;= \\frac{3}{r^3} - \\frac{3}{r^2}(r_x^2 + r_y^2 + r_z^2) = 0. \\end{align*} $$\nSo $D_{ij}$ is traceless. If we diagonalize, we\u0026rsquo;ll get\n$$ \\begin{align} D_{ij} = \\begin{pmatrix} D_x \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; D_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; D_z \\end{pmatrix}, \\end{align} $$\nwhere $D_x + D_y + D_z = 0.$\nNow things will get tricky for a bit. Because the Silicon Vacancy $V_{\\text{Si}}^- \\in C_{3V}$ point group, we have $D_x = D_y$ symmetry. Something about the $C_{3V}$ crystal structure makes the $D$ tensor symmetric along $x \\leftrightarrow y$ directions; not sure. Regardless, we can parameterize now to fewer variables:\n$$ \\begin{cases} 2D_x + D_z = 0 \\\\ D_x = D_y \\end{cases} \\quad \\Rightarrow \\quad D_x = D_y = -\\frac{1}{2}D_z. $$\nDefine scalars $D, E$ such that\n$$ \\begin{align} D \u0026amp;= \\frac{3}{2}D_z \\\\ E \u0026amp;= \\frac{1}{2}(D_x - D_y) \\end{align} $$\n$D$ measures how much the crystal field stretches or compresses along the $z$ axis. $E$ measures how much the environment deviates from perfect axial symmetry. In practice, $E \\simeq 0$.\nNow the matrix in Equation (14) only has two unknowns:\n$$ D_{\\text{diag}} = \\begin{pmatrix} D_x \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; D_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; D_z \\end{pmatrix} = \\begin{pmatrix} -\\frac{D}{3} + E \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -\\frac{D}{3} - E \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{2}{3}D \\end{pmatrix}. $$\nRaising and Lowering# From Equation (13), $\\hat{H}_{ZFS} = \\hat{\\vec{S}} \\cdot D \\cdot \\hat{\\vec{S}}$. We can expand,\n$$ \\begin{align*} \\hat{H}_{ZFS} \u0026amp;= \\hat{\\vec{S}} \\cdot D \\cdot \\hat{\\vec{S}} \\\\ \u0026amp;= \\left(-\\frac{D}{3} + E\\right) \\hat{S}_x^2 - \\left( \\frac{D}{3} + E \\right) \\hat{S}_y^2 + \\frac{2}{3}D \\hat{S}_z^2 \\\\ \u0026amp;= D \\left( \\hat{S}_z^2 - \\frac{1}{3} \\left( \\hat{S}_x^2 + \\hat{S}_y^2 + \\hat{S}_z^2 \\right) \\right) + E(\\hat{S}_x^2 - \\hat{S}_y^2), \\end{align*} $$\nwith the $\\hat{S}_x, \\hat{S}_y$ Cartesian spin operators being related to the spin raising and lowering operators via\n$$ \\begin{align} \\hat{S}_{+} |s, m\\rangle \u0026amp;= (\\hat{S}_x + i\\hat{S}_y ) |s, m\\rangle \\\\ \u0026amp;= \\sqrt{(s(s+1) - m(m+1))}|s, m+1\\rangle \\\\ \\hat{S}_{-} |s, m\\rangle \u0026amp;= (\\hat{S}_x - i\\hat{S}_y ) |s, m\\rangle \\\\ \u0026amp;= \\sqrt{(s(s+1) - m(m-1))}|s, m-1\\rangle. \\end{align} $$\nFinally, putting everything together gives a general expression for $\\hat{H}_{ZFS}$ acting on a general $|s, m\\rangle$:\n$$\\boxed{ \\begin{align*} \\hat{H}_{ZFS}|s, m\\rangle \u0026amp;= Dm^2 |s, m\\rangle - \\frac{D}{3}(s(s+1))|s, m\\rangle \\\\ \u0026amp;+ \\frac{E}{2} (s(s+1) - m(m+1))|s, m+2\\rangle \\\\ \u0026amp;+ \\frac{E}{2} (s(s+1) - m(m-1))|s, m-2\\rangle. \\end{align*}} $$\n$\\hat{H}_{ZFS}$ splits energy levels slightly even without an external magnetic field \u0026ndash; there\u0026rsquo;s no $B$ term anywhere. However, it introduces forbidden $|s, m\\pm 2\\rangle$ transitions that cause a very faint \u0026ldquo;half-field\u0026rdquo; response in EDMR spectra.\nTo build a perfect simulation with the half-field response, we\u0026rsquo;ll need to treat ZFS with care.\n← Previous Next → "},{"id":8,"href":"/posts/nasa_derive_1/","title":"Zeeman and Hyperfine","section":"posts","content":" This post is math heavy. I\u0026rsquo;ll try to walk through it all elegantly though. The important equations are boxed if you\u0026rsquo;d like to skip through. On mobile some equations may go overfull.\nThe Spin Hamiltonian $\\mathscr{H}$ governs the spin-physics of recombination. It\u0026rsquo;s what we need to program before starting to simulate EDMR.\n$ \\mathscr{H} $ is a combination of the Zeeman Effect, Hyperfine Interactions, the Zero-Field Splitting Effect, and the Exchange Interaction.\n$$ \\mathscr{H} = \\hat{H}_{\\mathrm{Z}} + \\hat{H}_{\\mathrm{HF}} + \\hat{H}_{\\mathrm{ZFS}} + \\hat{H}_{\\mathrm{EX}} $$\nNormally $\\mathscr{H}$ also has a \u0026ldquo;Nuclear Quadrupole Interaction Hamiltonian\u0026rdquo; as well. But since Silicon Carbide (4H-SiC) has no nuclei with nuclear spin $I \u0026gt; 1/2$, it\u0026rsquo;s set to 0.\nThe goal of this post is to provide a concise derivation of the first two Hamiltonian terms. The next two are reviewed in the next post.\nI use a fancy $\\mathscr{H}$ to represent the full Spin Hamiltonian. Sub-Hamiltonians are normal $H$\u0026rsquo;s but with hats (e.g. $\\hat{H}_Z$).\nThe Zeeman Effect# $\\hat{H}_Z$ determines how spin-state energies split in the presence of an external $\\vec{B}$ field. It\u0026rsquo;s the dominant term in the Spin Hamiltonian that drives recombination.\nA spinning charged particle is a magnetic dipole. Its magnetic dipole moment, $\\vec{\\mu}$, is proportional to its spin angular momentum, $\\vec{S}$\n$$ \\vec{\\mu} = \\gamma \\vec{S}.$$\nThe proportionality constant, $\\gamma$ is the gyromagnetic ratio. From the Dirac equation it can be shown\n$$ \\vec{\\mu} = \\gamma \\vec{S} = -g \\frac{q}{2m_e} \\vec{S} = -\\frac{g\\mu_B}{\\hbar} \\vec{S}, $$\nwhere $g \\approx 2.0023$ is the Landé $g$-factor for the free electron and $\\mu_B \\approx 9.2 \\cdot 10^{-24} J/T $ is the Bohr Magneton.\nWhen a magnetic dipole is placed in a magnetic field $\\vec{B}$, it experiences a torque, $\\vec{\\mu} \\times \\vec{B}$, which tends to line it up parallel to the field like a compass. The energy associated with this torque is\n$$ \\begin{align} H = -\\vec{\\mu} \\cdot \\vec{B} = -\\gamma \\vec{B} \\cdot \\vec{S}. \\end{align} $$\nTherefore, the Zeeman Hamiltonian is\n$$ \\hat{H}_Z = -\\vec{\\mu} \\cdot \\vec{B}_0 = \\frac{g\\mu_B}{\\hbar} \\vec{S} \\cdot \\vec{B}_0 = \\frac{g\\mu_B}{\\hbar}B_0 S_z $$\nwhere $S_z = \\frac{\\hbar}{2} \\hat{\\sigma}_z$ and $\\sigma_z$ is the Pauli-$z$ spin matrix. Applying $\\hat{H}_Z$ on an arbitrary spin state $|s, m_s\\rangle$ gives\n$$ \\begin{align} \\hat{H}_Z |s, m_s\\rangle \u0026amp;= \\frac{g\\mu_B}{\\hbar}B_0 \\cdot S_z |s, m_s\\rangle \\\\ \u0026amp;= \\frac{g\\mu_B}{\\hbar}B_0 \\cdot m_s\\hbar|s, m_s\\rangle \\\\ \u0026amp;= m_s g \\mu_B B_0 |s, m_s\\rangle. \\end{align} $$\nFrom Equation (4) above, we can calculate $\\hat{H}_Z$\u0026rsquo;s effect on any spin state $|s, m_s\\rangle$.\nOrdinarily we use an anisotropic $g$-tensor. Magnetic fields in different directions act differently on $|s, m_s\\rangle$. So we would replace with a $3 \\times 3$ $g$ tensor,\n$$\\hat{H}_Z = \\mu_B \\vec{S} \\cdot g \\cdot \\vec{B}_0.$$\nThis can be diagonalized along $|s, m_s\\rangle$\u0026rsquo;s principle axis to make things easier.\n$$ g_{\\mathrm{diag}} = \\begin{pmatrix} g_x \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; g_y \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; g_z \\end{pmatrix}. $$\nFor our simulation though, we\u0026rsquo;ll start with $g \\approx 2.0023$. Our Zeeman Hamiltonian is given by\n$$ \\boxed { \\hat{H}_Z |s, m_s\\rangle = m_s g \\mu_B B_0 |s, m_s\\rangle. } $$\nHyperfine Interaction# The hyperfine interaction comes from the magnetic field generated by the nucleus $\\vec{B}(\\vec{r})$ acting on the magnetic moment of an orbiting electron. We will calculate $\\vec{B}(\\vec{r})$ and use Equation (1) to calculate $\\hat{H}_{HF}$.\nMaxwell\u0026rsquo;s Equations say\n$$ \\nabla \\cdot \\vec{B} = 0, \\quad \\nabla \\times \\vec{B} = \\mu_0 \\vec{J}. $$\nVector Potential# The first equation tells us there\u0026rsquo;s no magnetic monopoles; the magnetic field $\\vec{B}$ has zero divergence. From vector calculus this means there exists a vector field $\\vec{A}$ such that\n$$ \\vec{B} = \\nabla \\times \\vec{A}. $$\n$A$ isn\u0026rsquo;t unique, but to make math simple we can impose the Coulomb gauge condition $ \\nabla \\cdot \\vec{A} = 0.$ Substituting yields\n$$ \\begin{align} \\nabla^2 \\vec{A} = -\\mu_0 \\vec{J}. \\end{align} $$\nCurrent Density# $\\vec{J}$ is called the \u0026ldquo;current density.\u0026rdquo; We know current is the amount of charge per unit time that travels through a wire (i.e. electron flux). If electrons aren\u0026rsquo;t trapped in a wire, they flow throughout space. At every point in this space we can assign a small vector that says\n\u0026ldquo;Here\u0026rsquo;s how much charge is flowing through this point, and in what direction.\u0026rdquo;\nThis vector field is the current density $\\vec{J}$. If we model the localized nuclear magnetic moment $\\vec{\\mu}_n$ at the origin, it generates an effective current density\n$$ \\vec{J}_n(\\vec{r}\u0026rsquo;) = \\nabla\u0026rsquo; \\times [\\vec{\\mu}_n \\delta^3 (\\vec{r}\u0026rsquo;)]. $$\nThe general solution for $\\vec{A}$ in Equation (5) is\n$$ \\vec{A}(\\vec{r}) = \\frac{\\mu_0}{4\\pi} \\int d^3 r\u0026rsquo; \\, \\frac{\\vec{J}_n (\\vec{r}\u0026rsquo;)}{|\\vec{r} - \\vec{r}\u0026rsquo;|} = \\frac{\\mu_0}{4\\pi} \\vec{\\mu}_n \\times \\nabla \\left( \\frac{1}{r} \\right) = \\frac{\\mu_0}{4\\pi}\\frac{\\vec{\\mu}_n \\times \\vec{r}}{r^3}. $$\nThen, using the identity\n$$ \\nabla \\times (\\vec{a} \\times \\vec{r}f(r)) = \\vec{a} \\nabla \\cdot (\\vec{r}f) - (\\vec{a} \\cdot \\nabla)(\\vec{r} f), $$\nwith $\\vec{a} = \\vec{\\mu}_n$ and $f(r) = 1/r^3$, we get the canonical expression for a magnetic field from a dipole in classical electrodynamics:\n$$ \\begin{align} \\vec{B}(\\vec{r}) = \\frac{\\mu_0}{4\\pi r^3} \\left[ 3(\\vec{\\mu}_n \\cdot \\hat{r}) \\hat{r} - \\vec{\\mu}_n \\right] + \\frac{2\\mu_0}{3}\\vec{\\mu}_n \\delta^3 (\\vec{r}). \\end{align} $$\nHamiltonian Density# A magnetic moment $\\vec{\\mu}_e$ at $\\vec{r}$ has energy (Equation (1))\n$$ U(\\vec{r}) = -\\vec{\\mu}_e \\cdot \\vec{B}(\\vec{r}). $$\nSubstituting $\\vec{B}(\\vec{r})$ with the expression in Equation (6), we can separate the energy into two terms:\n$$ \\begin{align} U_{\\text{dipolar}}(\\vec{r}) \u0026amp;= -\\vec{\\mu}_e \\cdot \\left[ \\frac{\\mu_0}{4\\pi r^3} (3(\\vec{\\mu}_n \\cdot \\hat{r}) \\hat{r} - \\vec{\\mu}_n) \\right],\\\\ U_{\\text{contact}}(\\vec{r}) \u0026amp;= -\\vec{\\mu}_e \\cdot \\left[ \\frac{2\\mu_0}{3} \\vec{\\mu}_n \\delta^3 (\\vec{r})\\right]. \\end{align} $$\nPromoting magnetic moments to their \u0026ldquo;quantum operators\u0026rdquo;,\n$$ \\begin{align} \\vec{\\mu}_e \u0026amp;= -g_e \\mu_B \\frac{\\hat{S}}{\\hbar}, \\\\ \\vec{\\mu}_n \u0026amp;= +g_n \\mu_N \\frac{\\hat{I}}{\\hbar}, \\end{align} $$\nwhere the Bohr Magneton $\\mu_B = e\\hbar / (2m_e)$ and the Nuclear Magneton $\\mu_N = e\\hbar / (2m_p)$. Now we can substitute everything into Equation (1) to calculate the Hamiltonian of the electron, in the magnetic field generated from the nuclei\u0026rsquo;s magnetic dipole moment at $\\vec{r}$.\n$$ H(\\vec{r}) = \\frac{\\mu_0}{4\\pi} \\frac{g_e \\mu_B g_n \\mu_N}{\\hbar^2} \\left( \\frac{\\vec{S} \\cdot \\vec{I} - 3(\\vec{S} \\cdot \\hat{r})(\\vec{I} \\cdot \\hat{r})}{r^3} + \\frac{8\\pi}{3} \\delta^3(\\vec{r}) \\vec{S} \\cdot \\vec{I} \\right). $$\nThe first term before the parenthesis is just a constant $C$. If an electron occupies a spatial wave function, $\\psi(\\vec{r})$, the full Hyperfine Hamiltonian is\n$$ \\hat{H}_{HF} = \\int d^3 r \\, |\\psi(\\vec{r})|^2 H(\\vec{r}). $$\nNow we can solve. We\u0026rsquo;ll write the Hamiltonian in index form,\n$$ H(\\vec{r}) = C \\left[ \\frac{1}{r^3}(\\delta_{kl} - 3\\hat{r}_k \\hat{r}_l) S_k I_l + \\frac{8\\pi}{3}\\delta^3 (\\vec{r}) S_k I_k \\right], $$\nwhere we sum over repeated indices. Now the final expression to solve is\n$$ \\hat{H}_{HF} = C \\left[ \\underbrace{S_k I_l \\int d^3 |\\psi(\\vec{r})|^2 \\frac{\\delta_{kl} - 3\\hat{r}_k\\hat{r}_l}{r^3}}_{\\text{dipolar term}} + \\underbrace{\\frac{8\\pi}{3}S_k I_k \\int d^3 r |\\psi(\\vec{r})|^2 \\delta^3 (\\vec{r})}_{\\text{contact (isotropic) term}} \\right]. $$\nThe contact term reduces to\n$$ \\begin{align} \\frac{8\\pi}{3} S_k I_k \\int d^3 |\\psi(\\vec{r})|^2 \\delta^3(\\vec{r}) \u0026amp;= \\frac{8\\pi}{3} |\\psi(0)|^2 S_k I_k \\\\ \u0026amp;= A_{\\text{iso}} \\delta_kl S_k I_l, \\end{align} $$\nwhere $A_{\\text{iso}} = C \\frac{8\\pi}{3}|\\psi(0)|^2. $ To solve the dipolar term, we define\n$$ A_{kl}^{\\text{dip}} = C \\int d^3 r |\\psi(\\vec{r})|^2 \\frac{\\delta_{kl} - 3\\hat{r}_k \\hat{r}_l}{r^3}. $$\nThen the dipolar term reduces to $S_k I_l A_{kl}^{\\text{dip}}.$ We define the $A_{ij}$ tensor as\n$$ A_{ij} = A_{\\text{iso}} \\delta_{ij} + A_{ij}^{\\text{dip}}. $$\nFinally,\n$$ \\boxed{ \\hat{H}_{HF} = \\hat{S} \\cdot A \\cdot \\hat{I} = \\hat{S}_i A_{ij} \\hat{I}_j. } $$\nFor multiple nuclei, like in Silicon Carbide, we sum over every $j$ inequivalent nucleus.\n$$ \\hat{H}_{HF} = \\sum_j \\hat{\\vec{S}} \\cdot A_j \\cdot \\hat{\\vec{I}}_j. $$\nFor a 2-nuclei (Silicon + Carbon) $\\times$ 2-electron (Defect + Carrier) system, the Hilbert space is $2^4 = 16$ dimensional. The Hyperfine Hamiltonian is a $16\\times 16$ matrix.\nMaking a $B_0$ vs. Energy plot for Hyperfine like with Zeeman above is difficult. However, in a future post, we\u0026rsquo;ll generate it. I will be writing some \u0026ldquo;eigen-energy\u0026rdquo; simulations. These will let us see what the Hyperfine Interaction does to the 16 basis states defining our spin-system at play.\n← Previous Next → "},{"id":9,"href":"/posts/nasa_intro/","title":"My project at NASA","section":"posts","content":" My work revolves around Electrically Detected Magnetic Resonance (EDMR). It\u0026rsquo;s a method to detect small magnetic fields electrically. In principle it\u0026rsquo;s very simple.\nSay we have a semiconductor, like silicon carbide. This semiconductor, made by nature or in a lab, is never perfect. There may exist some missing atoms, or extra atoms, deep in its molecular structure. These defects provide some extra electrons, or extra holes, which can be used for quantum sensing. In EDMR, they are used to measure magnetic fields electrically. I\u0026rsquo;ll now walk through how this is done in silicon carbide (4H-SiC) specifically.\nDefects in Silicon Carbide# Let\u0026rsquo;s look at the molecular structure of 4H-SiC.\nIdeally, it\u0026rsquo;s consistently full of silicon and carbon. However, what if somewhere there\u0026rsquo;s a missing silicon atom (middle of figure)? This is a common defect, known as the Silicon Vacancy. It\u0026rsquo;s a spin $S=3/2$ defect; how we get 3/2 isn\u0026rsquo;t important.\nEnergy Levels# When current flows through a semiconductor, the carrier electrons move through the conduction band, a band of energy a bit higher than where the valence electrons of silicon and carbon rest. The energy level of the defect lies somewhere in between.\nIn 4H-SiC, the conduction band is $\\sim$3.3eV higher than the valence band. If an electron ($S=1/2$) in the conduction band passes over a defect, the defect can trap it at its lower energy state.\nRecombination# Now the carrier electron is at the defect energy level, a bit below the conduction band. When can it travel all the way down to the valence band?\nElectron spin is an angular momentum. If an electron were to travel to the valence band then it would occupy a hole. This final state would have spin $S=0$. By angular momentum conservation, the defect-electron state before must have spin $S=0$ too.\nFor two particles (defect + electron), this is the singlet state, when both spins are antiparallel. A carrier electron can only undergo recombination if it forms a singlet state with the defect. If the carrier electron forms a triplet state, ($S = 1$), it immediately returns back up to the conduction band.\nThis is known as Spin-Dependent Recombination (SDR), illustrated below.\nCurrent# When an electron undergoes recombination, it fully leaves the conduction band. Therefore, current decreases.\nAnd since magnetic fields alter spin alignment (Zeeman precession), they change the rate of recombination, so they change current.\nIf we detect changes in current, we detect changes in the magnetic field. This is EDMR.\nThere are other mechanisms at play besides recombination though. Defects also provide a intermediary path for electrons to \u0026ldquo;hop\u0026rdquo; through to lower energy levels. This also lets them escape the conduction band, changing current. This is known as Spin-Dependent Trap-Assisted Tunneling (SDTAT), and is detectable by EDMR. However, an electron quantum tunneling is rarer than undergoing recombination, so recombination is the primary mechanism for EDMR.\nThe Goal# My work at NASA will be simulating this entire process, as fast as possible. The final simulation will take in all experimental parameters, to be explained later, and spit out an EDMR spectra. This spectra is a plot of magnetic field vs. current.\nI am told the use-case of this simulation will be to develop an automatic GPS aboard aircrafts that uses the Earth\u0026rsquo;s magnetic field. If a pilot\u0026rsquo;s GPS fails, a simulation could continuously predict EDMR spectra based on the surrounding environment. Different locations on Earth have different ambient magnetic fields, so a makeshift GPS is possible. I don\u0026rsquo;t think this idea is necessarily applicable in practice. There are better ways of error-handling a GPS failure. However, it\u0026rsquo;s a fun project.\nIn any case, we may understand spin-physics a bit more. This post was a bit abstracted. For the next few posts, I plan to cover the core physics of spin-dependent recombination and then start writing software to simulate it.\nNext → "},{"id":10,"href":"/resume/","title":"resume","section":"my undergrad work","content":" Can’t display the PDF. Download.\n"}]